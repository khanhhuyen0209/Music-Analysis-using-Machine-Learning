{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jmLlm66KeeY",
        "outputId": "5139bf6e-a046-4915-831d-5c986923d682",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: (944904, 64)\n",
            "y: (944904,)\n",
            "After voiced-only:\n",
            "X: (618376, 64)\n",
            "y: (618376,)\n"
          ]
        }
      ],
      "source": [
        "#@title Load pitch dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = np.load(\"pitch_tempo.npz\")\n",
        "\n",
        "X = data[\"X\"]          # (N, 64) log-mel, đã scale\n",
        "y = data[\"pitch\"]      # (N,) pitch class [0..50]\n",
        "\n",
        "print(\"X:\", X.shape)\n",
        "print(\"y:\", y.shape)\n",
        "\n",
        "# bỏ unvoiced (class = 0) nếu muốn\n",
        "mask = y > 0\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "print(\"After voiced-only:\")\n",
        "print(\"X:\", X.shape)\n",
        "print(\"y:\", y.shape)\n",
        "\n",
        "NUM_CLASSES = int(y.max() + 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export mean and scale for TinyML\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "mean = scaler.mean_\n",
        "scale = scaler.scale_\n",
        "\n",
        "print(\"Mean:\")\n",
        "print(mean)\n",
        "\n",
        "print(\"\\nScale (std):\")\n",
        "print(scale)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2rn7sFct2up",
        "outputId": "f58d00d4-fa6b-4381-a42e-4dae63896a6b",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            "[-31.47932516 -31.619726   -34.58918788 -31.43537355 -29.15595713\n",
            " -29.38718413 -31.50365041 -33.58297018 -34.13655313 -34.96699259\n",
            " -36.43684129 -37.82619215 -39.41558779 -40.53129804 -42.09631899\n",
            " -42.42878572 -43.5009231  -46.13483026 -48.68916782 -49.24920747\n",
            " -49.13421082 -51.26572491 -52.63556974 -52.29285503 -53.11413171\n",
            " -52.14604688 -52.12687569 -51.38349772 -52.05142022 -50.65060592\n",
            " -50.57307541 -52.74523542 -54.91302744 -54.85145829 -56.98773544\n",
            " -53.41452484 -53.41389753 -51.60816136 -52.3967419  -52.68249512\n",
            " -51.82719158 -52.67357656 -53.2603045  -55.27142623 -54.17020182\n",
            " -57.95720256 -56.94443876 -58.20819817 -57.87664787 -57.94370646\n",
            " -57.26015645 -57.07221509 -58.74828378 -57.61877876 -57.02966454\n",
            " -56.835714   -56.72278572 -56.50173418 -55.72662125 -56.0367207\n",
            " -56.04366191 -55.51121848 -54.8771185  -56.70047583]\n",
            "\n",
            "Scale (std):\n",
            "[13.35365875 13.69935291 15.74900158 15.6477659  15.31074646 15.5268877\n",
            " 15.59203407 14.83102993 14.54253457 15.16858525 15.53326505 15.28363189\n",
            " 15.51706467 15.9009896  15.71509448 14.95779785 15.02571186 15.47151084\n",
            " 15.92347857 16.13870384 15.70969493 15.90199158 14.94252739 14.36264607\n",
            " 14.80975917 14.97840065 13.95066576 15.16707188 15.17213327 15.78063226\n",
            " 15.05557797 14.43866076 14.38127501 14.50584364 14.73796606 14.81222148\n",
            " 15.33217285 15.19824461 16.32967383 16.63241848 16.05629822 16.57244621\n",
            " 15.69292181 16.22750633 14.86925324 14.76680832 14.2011591  14.49847376\n",
            " 14.95698867 13.57675454 15.03158274 13.55913212 14.21718687 13.24097547\n",
            " 14.76442546 14.82053051 14.83564059 15.87173663 14.41766203 15.8803952\n",
            " 14.30585901 16.48745214 15.72730351 14.79999728]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert mean and scale to C array\n",
        "def to_c_array(name, arr, per_line=8):\n",
        "    print(f\"static const float {name}[{len(arr)}] = {{\")\n",
        "    for i in range(0, len(arr), per_line):\n",
        "        chunk = \", \".join(f\"{x:.8f}f\" for x in arr[i:i+per_line])\n",
        "        print(\"  \" + chunk + (\",\" if i + per_line < len(arr) else \"\"))\n",
        "    print(\"};\\n\")\n",
        "\n",
        "to_c_array(\"PITCH_MEAN\", scaler.mean_)\n",
        "to_c_array(\"PITCH_SCALE\", scaler.scale_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_HMfVzDvOJW",
        "outputId": "1e9f72fa-e9e7-46d8-9295-f9f29a4154a1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "static const float PITCH_MEAN[64] = {\n",
            "  -31.47932516f, -31.61972600f, -34.58918788f, -31.43537355f, -29.15595713f, -29.38718413f, -31.50365041f, -33.58297018f,\n",
            "  -34.13655313f, -34.96699259f, -36.43684129f, -37.82619215f, -39.41558779f, -40.53129804f, -42.09631899f, -42.42878572f,\n",
            "  -43.50092310f, -46.13483026f, -48.68916782f, -49.24920747f, -49.13421082f, -51.26572491f, -52.63556974f, -52.29285503f,\n",
            "  -53.11413171f, -52.14604688f, -52.12687569f, -51.38349772f, -52.05142022f, -50.65060592f, -50.57307541f, -52.74523542f,\n",
            "  -54.91302744f, -54.85145829f, -56.98773544f, -53.41452484f, -53.41389753f, -51.60816136f, -52.39674190f, -52.68249512f,\n",
            "  -51.82719158f, -52.67357656f, -53.26030450f, -55.27142623f, -54.17020182f, -57.95720256f, -56.94443876f, -58.20819817f,\n",
            "  -57.87664787f, -57.94370646f, -57.26015645f, -57.07221509f, -58.74828378f, -57.61877876f, -57.02966454f, -56.83571400f,\n",
            "  -56.72278572f, -56.50173418f, -55.72662125f, -56.03672070f, -56.04366191f, -55.51121848f, -54.87711850f, -56.70047583f\n",
            "};\n",
            "\n",
            "static const float PITCH_SCALE[64] = {\n",
            "  13.35365875f, 13.69935291f, 15.74900158f, 15.64776590f, 15.31074646f, 15.52688770f, 15.59203407f, 14.83102993f,\n",
            "  14.54253457f, 15.16858525f, 15.53326505f, 15.28363189f, 15.51706467f, 15.90098960f, 15.71509448f, 14.95779785f,\n",
            "  15.02571186f, 15.47151084f, 15.92347857f, 16.13870384f, 15.70969493f, 15.90199158f, 14.94252739f, 14.36264607f,\n",
            "  14.80975917f, 14.97840065f, 13.95066576f, 15.16707188f, 15.17213327f, 15.78063226f, 15.05557797f, 14.43866076f,\n",
            "  14.38127501f, 14.50584364f, 14.73796606f, 14.81222148f, 15.33217285f, 15.19824461f, 16.32967383f, 16.63241848f,\n",
            "  16.05629822f, 16.57244621f, 15.69292181f, 16.22750633f, 14.86925324f, 14.76680832f, 14.20115910f, 14.49847376f,\n",
            "  14.95698867f, 13.57675454f, 15.03158274f, 13.55913212f, 14.21718687f, 13.24097547f, 14.76442546f, 14.82053051f,\n",
            "  14.83564059f, 15.87173663f, 14.41766203f, 15.88039520f, 14.30585901f, 16.48745214f, 15.72730351f, 14.79999728f\n",
            "};\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train / Val split + downsample\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Downsample train set\n",
        "MAX_TRAIN = 200000\n",
        "idx = np.random.choice(len(X_tr), MAX_TRAIN, replace=False)\n",
        "\n",
        "X_tr = X_tr[idx]\n",
        "y_tr = y_tr[idx]\n",
        "\n",
        "print(\"Train:\", X_tr.shape)\n",
        "print(\"Val:\", X_va.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0351_doMz56",
        "outputId": "82651248-a8ba-471c-e12c-7a62f7950da2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (200000, 64)\n",
            "Val: (123676, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train teacher model\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "teacher = MLPClassifier(\n",
        "    hidden_layer_sizes=(512, 512, 256),\n",
        "    activation=\"relu\",\n",
        "    solver=\"adam\",\n",
        "    batch_size=1024,\n",
        "    max_iter=20,\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "teacher.fit(X_tr, y_tr)\n",
        "\n",
        "y_pred = teacher.predict(X_va)\n",
        "print(\"Teacher Val ACC:\", accuracy_score(y_va, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "HHuSNUooNwuH",
        "outputId": "a2f8d28a-897a-4a97-cae1-d206b35e016a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.08548552\n",
            "Iteration 2, loss = 1.08500554\n",
            "Iteration 3, loss = 0.99435794\n",
            "Iteration 4, loss = 0.93958723\n",
            "Iteration 5, loss = 0.89683094\n",
            "Iteration 6, loss = 0.86163016\n",
            "Iteration 7, loss = 0.83205723\n",
            "Iteration 8, loss = 0.81132963\n",
            "Iteration 9, loss = 0.79039115\n",
            "Iteration 10, loss = 0.77546890\n",
            "Iteration 11, loss = 0.75723983\n",
            "Iteration 12, loss = 0.73441222\n",
            "Iteration 13, loss = 0.72844275\n",
            "Iteration 14, loss = 0.71359600\n",
            "Iteration 15, loss = 0.69948239\n",
            "Iteration 16, loss = 0.68741290\n",
            "Iteration 17, loss = 0.67780142\n",
            "Iteration 18, loss = 0.66728184\n",
            "Iteration 19, loss = 0.65770030\n",
            "Iteration 20, loss = 0.64974706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Val ACC: 0.7887787444613344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate soft labels from teacher\n",
        "\n",
        "T = 4.0  # temperature\n",
        "\n",
        "logits = teacher.predict_proba(X_tr)\n",
        "soft_labels = np.log(logits + 1e-9) / T\n",
        "soft_labels = np.exp(soft_labels)\n",
        "soft_labels /= soft_labels.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(\"Soft labels:\", soft_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "GPOP5U74PRYi",
        "outputId": "9117e226-4928-441e-d434-f0bdba098a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soft labels: (200000, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train student model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "NUM_CLASSES = soft_labels.shape[1]\n",
        "input_shape = X_tr.shape[1]\n",
        "\n",
        "student = models.Sequential([\n",
        "    layers.Input(shape=(input_shape,)),\n",
        "    layers.Dense(64, activation='tanh'),\n",
        "    layers.Dense(32, activation='tanh'),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "student.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "student.fit(\n",
        "    X_tr, soft_labels,\n",
        "    validation_data=(X_va, teacher.predict_proba(X_va)),\n",
        "    batch_size=512,\n",
        "    epochs=30\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bkWW04dVPWZa",
        "outputId": "3ff4815b-e9f9-41d9-e7c4-efbe8fae2f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.2295 - loss: 3.5393 - val_accuracy: 0.4872 - val_loss: 2.3988\n",
            "Epoch 2/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5211 - loss: 3.2149 - val_accuracy: 0.5661 - val_loss: 2.1823\n",
            "Epoch 3/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5771 - loss: 3.1634 - val_accuracy: 0.5871 - val_loss: 2.1209\n",
            "Epoch 4/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5959 - loss: 3.1420 - val_accuracy: 0.6053 - val_loss: 2.0911\n",
            "Epoch 5/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6123 - loss: 3.1264 - val_accuracy: 0.6185 - val_loss: 2.0717\n",
            "Epoch 6/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6225 - loss: 3.1192 - val_accuracy: 0.6227 - val_loss: 2.0437\n",
            "Epoch 7/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6281 - loss: 3.1129 - val_accuracy: 0.6366 - val_loss: 2.0273\n",
            "Epoch 8/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6343 - loss: 3.1051 - val_accuracy: 0.6407 - val_loss: 2.0013\n",
            "Epoch 9/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6405 - loss: 3.1019 - val_accuracy: 0.6426 - val_loss: 2.0066\n",
            "Epoch 10/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6426 - loss: 3.0956 - val_accuracy: 0.6447 - val_loss: 2.0454\n",
            "Epoch 11/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6493 - loss: 3.0918 - val_accuracy: 0.6525 - val_loss: 1.9854\n",
            "Epoch 12/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6529 - loss: 3.0878 - val_accuracy: 0.6550 - val_loss: 1.9692\n",
            "Epoch 13/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6533 - loss: 3.0889 - val_accuracy: 0.6593 - val_loss: 1.9583\n",
            "Epoch 14/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6578 - loss: 3.0861 - val_accuracy: 0.6646 - val_loss: 1.9376\n",
            "Epoch 15/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6587 - loss: 3.0843 - val_accuracy: 0.6665 - val_loss: 1.9536\n",
            "Epoch 16/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6607 - loss: 3.0808 - val_accuracy: 0.6685 - val_loss: 1.9392\n",
            "Epoch 17/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6637 - loss: 3.0796 - val_accuracy: 0.6634 - val_loss: 1.9506\n",
            "Epoch 18/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6642 - loss: 3.0785 - val_accuracy: 0.6673 - val_loss: 1.9528\n",
            "Epoch 19/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6669 - loss: 3.0743 - val_accuracy: 0.6714 - val_loss: 1.9432\n",
            "Epoch 20/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6690 - loss: 3.0755 - val_accuracy: 0.6706 - val_loss: 1.9502\n",
            "Epoch 21/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6699 - loss: 3.0747 - val_accuracy: 0.6727 - val_loss: 1.9191\n",
            "Epoch 22/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6725 - loss: 3.0728 - val_accuracy: 0.6723 - val_loss: 1.9627\n",
            "Epoch 23/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6739 - loss: 3.0725 - val_accuracy: 0.6694 - val_loss: 1.9327\n",
            "Epoch 24/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6744 - loss: 3.0703 - val_accuracy: 0.6790 - val_loss: 1.9004\n",
            "Epoch 25/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6758 - loss: 3.0712 - val_accuracy: 0.6778 - val_loss: 1.9275\n",
            "Epoch 26/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.6743 - loss: 3.0733 - val_accuracy: 0.6797 - val_loss: 1.9093\n",
            "Epoch 27/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6771 - loss: 3.0694 - val_accuracy: 0.6767 - val_loss: 1.9130\n",
            "Epoch 28/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6760 - loss: 3.0691 - val_accuracy: 0.6785 - val_loss: 1.9264\n",
            "Epoch 29/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6769 - loss: 3.0690 - val_accuracy: 0.6782 - val_loss: 1.9300\n",
            "Epoch 30/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6789 - loss: 3.0693 - val_accuracy: 0.6789 - val_loss: 1.9442\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f6e262d3a10>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Baseline small MLP (no distillation)\n",
        "\n",
        "baseline = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32),\n",
        "    activation=\"tanh\",\n",
        "    max_iter=30,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "baseline.fit(X_tr, y_tr)\n",
        "pred = baseline.predict(X_va)\n",
        "\n",
        "print(\"Baseline ACC:\", accuracy_score(y_va, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Ty-acmi9QgnQ",
        "outputId": "0dcb2820-7e5a-408d-bac6-bb5c5ed4c073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline ACC: 0.6702513017885443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Finetune teacher model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "teacher_configs = [\n",
        "    (512, 512, 256),\n",
        "    (256, 256, 128),\n",
        "    (512, 256, 128),\n",
        "    (256, 128, 64)\n",
        "]\n",
        "activations = [\"relu\", \"tanh\"]\n",
        "\n",
        "best_val_acc_teacher = 0\n",
        "best_teacher = None\n",
        "best_teacher_cfg = None\n",
        "\n",
        "for cfg in teacher_configs:\n",
        "    for act in activations:\n",
        "        print(\"\\nTraining teacher with hidden layers:\", cfg, \"activation:\", act)\n",
        "        teacher = MLPClassifier(\n",
        "            hidden_layer_sizes=cfg,\n",
        "            activation=act,\n",
        "            solver=\"adam\",\n",
        "            batch_size=1024,\n",
        "            max_iter=30,\n",
        "            random_state=42,\n",
        "            verbose=True\n",
        "        )\n",
        "        teacher.fit(X_tr, y_tr)\n",
        "        y_pred = teacher.predict(X_va)\n",
        "        val_acc = accuracy_score(y_va, y_pred)\n",
        "        print(\"Validation ACC:\", val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc_teacher:\n",
        "            best_val_acc_teacher = val_acc\n",
        "            best_teacher = teacher\n",
        "            best_teacher_cfg = (cfg, act)\n",
        "\n",
        "print(\"\\nBest teacher ACC:\", best_val_acc_teacher, \"Config:\", best_teacher_cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "vZpUetD9TilW",
        "outputId": "162eb296-8f34-42a7-ae02-f3428e786a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training teacher with hidden layers: (512, 512, 256) activation: relu\n",
            "Iteration 1, loss = 2.08548552\n",
            "Iteration 2, loss = 1.08500554\n",
            "Iteration 3, loss = 0.99435794\n",
            "Iteration 4, loss = 0.93958723\n",
            "Iteration 5, loss = 0.89683094\n",
            "Iteration 6, loss = 0.86163016\n",
            "Iteration 7, loss = 0.83205723\n",
            "Iteration 8, loss = 0.81132963\n",
            "Iteration 9, loss = 0.79039115\n",
            "Iteration 10, loss = 0.77546890\n",
            "Iteration 11, loss = 0.75723983\n",
            "Iteration 12, loss = 0.73441222\n",
            "Iteration 13, loss = 0.72844275\n",
            "Iteration 14, loss = 0.71359600\n",
            "Iteration 15, loss = 0.69948239\n",
            "Iteration 16, loss = 0.68741290\n",
            "Iteration 17, loss = 0.67780142\n",
            "Iteration 18, loss = 0.66728184\n",
            "Iteration 19, loss = 0.65770030\n",
            "Iteration 20, loss = 0.64974706\n",
            "Iteration 21, loss = 0.64230746\n",
            "Iteration 22, loss = 0.63083667\n",
            "Iteration 23, loss = 0.61848188\n",
            "Iteration 24, loss = 0.61214731\n",
            "Iteration 25, loss = 0.60254083\n",
            "Iteration 26, loss = 0.59167045\n",
            "Iteration 27, loss = 0.58471286\n",
            "Iteration 28, loss = 0.58602224\n",
            "Iteration 29, loss = 0.57037575\n",
            "Iteration 30, loss = 0.56136839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.8016268313981694\n",
            "\n",
            "Training teacher with hidden layers: (512, 512, 256) activation: tanh\n",
            "Iteration 1, loss = 1.37320279\n",
            "Iteration 2, loss = 1.08179211\n",
            "Iteration 3, loss = 1.00457538\n",
            "Iteration 4, loss = 0.95489028\n",
            "Iteration 5, loss = 0.92557809\n",
            "Iteration 6, loss = 0.90312783\n",
            "Iteration 7, loss = 0.87776977\n",
            "Iteration 8, loss = 0.86027657\n",
            "Iteration 9, loss = 0.84851354\n",
            "Iteration 10, loss = 0.83227906\n",
            "Iteration 11, loss = 0.80940130\n",
            "Iteration 12, loss = 0.80552523\n",
            "Iteration 13, loss = 0.80256619\n",
            "Iteration 14, loss = 0.79150293\n",
            "Iteration 15, loss = 0.77720119\n",
            "Iteration 16, loss = 0.76964709\n",
            "Iteration 17, loss = 0.75972127\n",
            "Iteration 18, loss = 0.75183690\n",
            "Iteration 19, loss = 0.74657642\n",
            "Iteration 20, loss = 0.74175399\n",
            "Iteration 21, loss = 0.72773606\n",
            "Iteration 22, loss = 0.72882911\n",
            "Iteration 23, loss = 0.72165385\n",
            "Iteration 24, loss = 0.70982985\n",
            "Iteration 25, loss = 0.70670781\n",
            "Iteration 26, loss = 0.69854386\n",
            "Iteration 27, loss = 0.68255695\n",
            "Iteration 28, loss = 0.68897746\n",
            "Iteration 29, loss = 0.68309211\n",
            "Iteration 30, loss = 0.67315078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.7775397005077784\n",
            "\n",
            "Training teacher with hidden layers: (256, 256, 128) activation: relu\n",
            "Iteration 1, loss = 2.33621798\n",
            "Iteration 2, loss = 1.21456274\n",
            "Iteration 3, loss = 1.10774108\n",
            "Iteration 4, loss = 1.04512478\n",
            "Iteration 5, loss = 1.00165264\n",
            "Iteration 6, loss = 0.97313011\n",
            "Iteration 7, loss = 0.94194041\n",
            "Iteration 8, loss = 0.91454878\n",
            "Iteration 9, loss = 0.89628177\n",
            "Iteration 10, loss = 0.87576891\n",
            "Iteration 11, loss = 0.85821362\n",
            "Iteration 12, loss = 0.84369273\n",
            "Iteration 13, loss = 0.83304874\n",
            "Iteration 14, loss = 0.81730018\n",
            "Iteration 15, loss = 0.80472065\n",
            "Iteration 16, loss = 0.79865918\n",
            "Iteration 17, loss = 0.78735859\n",
            "Iteration 18, loss = 0.77488347\n",
            "Iteration 19, loss = 0.76805844\n",
            "Iteration 20, loss = 0.76274418\n",
            "Iteration 21, loss = 0.75432699\n",
            "Iteration 22, loss = 0.74945368\n",
            "Iteration 23, loss = 0.73587553\n",
            "Iteration 24, loss = 0.73288495\n",
            "Iteration 25, loss = 0.72183366\n",
            "Iteration 26, loss = 0.71779635\n",
            "Iteration 27, loss = 0.71131720\n",
            "Iteration 28, loss = 0.70477091\n",
            "Iteration 29, loss = 0.70269383\n",
            "Iteration 30, loss = 0.69624848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.7852453184126266\n",
            "\n",
            "Training teacher with hidden layers: (256, 256, 128) activation: tanh\n",
            "Iteration 1, loss = 1.58077913\n",
            "Iteration 2, loss = 1.17852096\n",
            "Iteration 3, loss = 1.09742080\n",
            "Iteration 4, loss = 1.04680324\n",
            "Iteration 5, loss = 1.00381716\n",
            "Iteration 6, loss = 0.97325030\n",
            "Iteration 7, loss = 0.95191224\n",
            "Iteration 8, loss = 0.93163005\n",
            "Iteration 9, loss = 0.91309808\n",
            "Iteration 10, loss = 0.89430497\n",
            "Iteration 11, loss = 0.88625468\n",
            "Iteration 12, loss = 0.86618109\n",
            "Iteration 13, loss = 0.85970082\n",
            "Iteration 14, loss = 0.84947184\n",
            "Iteration 15, loss = 0.84076360\n",
            "Iteration 16, loss = 0.82688193\n",
            "Iteration 17, loss = 0.82172988\n",
            "Iteration 18, loss = 0.81648525\n",
            "Iteration 19, loss = 0.80242793\n",
            "Iteration 20, loss = 0.80045171\n",
            "Iteration 21, loss = 0.79477160\n",
            "Iteration 22, loss = 0.79176809\n",
            "Iteration 23, loss = 0.78049736\n",
            "Iteration 24, loss = 0.77614983\n",
            "Iteration 25, loss = 0.77355421\n",
            "Iteration 26, loss = 0.76777115\n",
            "Iteration 27, loss = 0.76919737\n",
            "Iteration 28, loss = 0.75388738\n",
            "Iteration 29, loss = 0.75324916\n",
            "Iteration 30, loss = 0.74893186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.768039069827614\n",
            "\n",
            "Training teacher with hidden layers: (512, 256, 128) activation: relu\n",
            "Iteration 1, loss = 2.27007595\n",
            "Iteration 2, loss = 1.18516670\n",
            "Iteration 3, loss = 1.07213848\n",
            "Iteration 4, loss = 1.01519860\n",
            "Iteration 5, loss = 0.97341418\n",
            "Iteration 6, loss = 0.93881310\n",
            "Iteration 7, loss = 0.91292475\n",
            "Iteration 8, loss = 0.88778767\n",
            "Iteration 9, loss = 0.87073592\n",
            "Iteration 10, loss = 0.85554443\n",
            "Iteration 11, loss = 0.84015995\n",
            "Iteration 12, loss = 0.82603215\n",
            "Iteration 13, loss = 0.81508800\n",
            "Iteration 14, loss = 0.80128473\n",
            "Iteration 15, loss = 0.79237047\n",
            "Iteration 16, loss = 0.78218713\n",
            "Iteration 17, loss = 0.77496752\n",
            "Iteration 18, loss = 0.76531684\n",
            "Iteration 19, loss = 0.75905323\n",
            "Iteration 20, loss = 0.75102676\n",
            "Iteration 21, loss = 0.73977100\n",
            "Iteration 22, loss = 0.73515301\n",
            "Iteration 23, loss = 0.72497888\n",
            "Iteration 24, loss = 0.71743335\n",
            "Iteration 25, loss = 0.71341258\n",
            "Iteration 26, loss = 0.70543704\n",
            "Iteration 27, loss = 0.70397754\n",
            "Iteration 28, loss = 0.69457904\n",
            "Iteration 29, loss = 0.68510005\n",
            "Iteration 30, loss = 0.68175355\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.7881804068695624\n",
            "\n",
            "Training teacher with hidden layers: (512, 256, 128) activation: tanh\n",
            "Iteration 1, loss = 1.51173116\n",
            "Iteration 2, loss = 1.13726816\n",
            "Iteration 3, loss = 1.06478600\n",
            "Iteration 4, loss = 1.01600739\n",
            "Iteration 5, loss = 0.98750845\n",
            "Iteration 6, loss = 0.96185603\n",
            "Iteration 7, loss = 0.93599231\n",
            "Iteration 8, loss = 0.91476059\n",
            "Iteration 9, loss = 0.90717996\n",
            "Iteration 10, loss = 0.89206332\n",
            "Iteration 11, loss = 0.87802240\n",
            "Iteration 12, loss = 0.86649242\n",
            "Iteration 13, loss = 0.86286698\n",
            "Iteration 14, loss = 0.85016628\n",
            "Iteration 15, loss = 0.84038320\n",
            "Iteration 16, loss = 0.83556033\n",
            "Iteration 17, loss = 0.83184871\n",
            "Iteration 18, loss = 0.82623577\n",
            "Iteration 19, loss = 0.82418541\n",
            "Iteration 20, loss = 0.81542632\n",
            "Iteration 21, loss = 0.81499353\n",
            "Iteration 22, loss = 0.80752890\n",
            "Iteration 23, loss = 0.79918580\n",
            "Iteration 24, loss = 0.79953755\n",
            "Iteration 25, loss = 0.79255912\n",
            "Iteration 26, loss = 0.78264291\n",
            "Iteration 27, loss = 0.77868018\n",
            "Iteration 28, loss = 0.78235436\n",
            "Iteration 29, loss = 0.77156110\n",
            "Iteration 30, loss = 0.77581607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.7661793719072415\n",
            "\n",
            "Training teacher with hidden layers: (256, 128, 64) activation: relu\n",
            "Iteration 1, loss = 2.93392836\n",
            "Iteration 2, loss = 1.48127064\n",
            "Iteration 3, loss = 1.28927515\n",
            "Iteration 4, loss = 1.19691178\n",
            "Iteration 5, loss = 1.13973864\n",
            "Iteration 6, loss = 1.09817005\n",
            "Iteration 7, loss = 1.06639074\n",
            "Iteration 8, loss = 1.04042693\n",
            "Iteration 9, loss = 1.02025087\n",
            "Iteration 10, loss = 0.99938891\n",
            "Iteration 11, loss = 0.98549908\n",
            "Iteration 12, loss = 0.96173514\n",
            "Iteration 13, loss = 0.95142966\n",
            "Iteration 14, loss = 0.93865591\n",
            "Iteration 15, loss = 0.92631792\n",
            "Iteration 16, loss = 0.91622416\n",
            "Iteration 17, loss = 0.90186557\n",
            "Iteration 18, loss = 0.89556848\n",
            "Iteration 19, loss = 0.88810435\n",
            "Iteration 20, loss = 0.87505786\n",
            "Iteration 21, loss = 0.86825297\n",
            "Iteration 22, loss = 0.86335825\n",
            "Iteration 23, loss = 0.85304974\n",
            "Iteration 24, loss = 0.84630558\n",
            "Iteration 25, loss = 0.83879973\n",
            "Iteration 26, loss = 0.83259728\n",
            "Iteration 27, loss = 0.82750056\n",
            "Iteration 28, loss = 0.81983524\n",
            "Iteration 29, loss = 0.82272557\n",
            "Iteration 30, loss = 0.80948046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.7549807561693457\n",
            "\n",
            "Training teacher with hidden layers: (256, 128, 64) activation: tanh\n",
            "Iteration 1, loss = 1.83758717\n",
            "Iteration 2, loss = 1.26944036\n",
            "Iteration 3, loss = 1.17164194\n",
            "Iteration 4, loss = 1.12360003\n",
            "Iteration 5, loss = 1.08616006\n",
            "Iteration 6, loss = 1.05353065\n",
            "Iteration 7, loss = 1.02744562\n",
            "Iteration 8, loss = 1.00601125\n",
            "Iteration 9, loss = 0.98833718\n",
            "Iteration 10, loss = 0.97481088\n",
            "Iteration 11, loss = 0.96276821\n",
            "Iteration 12, loss = 0.95373769\n",
            "Iteration 13, loss = 0.94757520\n",
            "Iteration 14, loss = 0.93553848\n",
            "Iteration 15, loss = 0.92675426\n",
            "Iteration 16, loss = 0.92552785\n",
            "Iteration 17, loss = 0.91680703\n",
            "Iteration 18, loss = 0.91332831\n",
            "Iteration 19, loss = 0.90852079\n",
            "Iteration 20, loss = 0.89970875\n",
            "Iteration 21, loss = 0.89491765\n",
            "Iteration 22, loss = 0.89317546\n",
            "Iteration 23, loss = 0.88962301\n",
            "Iteration 24, loss = 0.88368651\n",
            "Iteration 25, loss = 0.88386771\n",
            "Iteration 26, loss = 0.87917807\n",
            "Iteration 27, loss = 0.87239521\n",
            "Iteration 28, loss = 0.87235902\n",
            "Iteration 29, loss = 0.86904631\n",
            "Iteration 30, loss = 0.86680114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation ACC: 0.7468870273941589\n",
            "\n",
            "Best teacher ACC: 0.8016268313981694 Config: ((512, 512, 256), 'relu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate soft labels from best teacher\n",
        "T = 4.0  # temperature\n",
        "\n",
        "logits = best_teacher.predict_proba(X_tr)\n",
        "soft_labels = np.log(logits + 1e-9) / T\n",
        "soft_labels = np.exp(soft_labels)\n",
        "soft_labels /= soft_labels.sum(axis=1, keepdims=True)\n",
        "\n",
        "val_logits = best_teacher.predict_proba(X_va)\n",
        "soft_labels_val = np.log(val_logits + 1e-9) / T\n",
        "soft_labels_val = np.exp(soft_labels_val)\n",
        "soft_labels_val /= soft_labels_val.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(\"Soft labels train:\", soft_labels.shape)\n",
        "print(\"Soft labels val:\", soft_labels_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "GyIVomywd7nI",
        "outputId": "377e6c63-7dd5-461d-c8d7-2830be3ce357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Soft labels train: (200000, 49)\n",
            "Soft labels val: (123676, 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Finetune student model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "NUM_CLASSES = soft_labels.shape[1]\n",
        "input_shape = X_tr.shape[1]\n",
        "\n",
        "student_configs = [\n",
        "    [64],\n",
        "    [64, 32],\n",
        "    [128, 64, 32]\n",
        "]\n",
        "activations_student = [\"tanh\", \"relu\"]\n",
        "\n",
        "best_val_acc_student = 0\n",
        "best_student = None\n",
        "best_student_cfg = None\n",
        "\n",
        "for cfg in student_configs:\n",
        "    for act in activations_student:\n",
        "        print(\"\\nTraining student with layers:\", cfg, \"activation:\", act)\n",
        "        student = models.Sequential()\n",
        "        student.add(layers.Input(shape=(input_shape,)))\n",
        "        for u in cfg:\n",
        "            student.add(layers.Dense(u, activation=act))\n",
        "        student.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "        student.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        history = student.fit(\n",
        "            X_tr, soft_labels,\n",
        "            validation_data=(X_va, soft_labels_val),\n",
        "            batch_size=512,\n",
        "            epochs=30,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        val_acc = max(history.history['val_accuracy'])\n",
        "        print(\"Student val ACC:\", val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc_student:\n",
        "            best_val_acc_student = val_acc\n",
        "            best_student = student\n",
        "            best_student_cfg = (cfg, act)\n",
        "\n",
        "print(\"\\nBest student val ACC:\", best_val_acc_student, \"Config:\", best_student_cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "V8-OLt5MeJIM",
        "outputId": "efa459d4-348b-478a-e5fe-c4a08e83ed31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training student with layers: [64] activation: tanh\n",
            "Epoch 1/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.1746 - loss: 3.5687 - val_accuracy: 0.4229 - val_loss: 3.2338\n",
            "Epoch 2/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.4598 - loss: 3.1845 - val_accuracy: 0.5386 - val_loss: 3.1010\n",
            "Epoch 3/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.5471 - loss: 3.0884 - val_accuracy: 0.5771 - val_loss: 3.0577\n",
            "Epoch 4/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5756 - loss: 3.0499 - val_accuracy: 0.5877 - val_loss: 3.0423\n",
            "Epoch 5/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5903 - loss: 3.0335 - val_accuracy: 0.5995 - val_loss: 3.0268\n",
            "Epoch 6/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5924 - loss: 3.0250 - val_accuracy: 0.6092 - val_loss: 3.0239\n",
            "Epoch 7/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.5985 - loss: 3.0168 - val_accuracy: 0.6026 - val_loss: 3.0176\n",
            "Epoch 8/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6004 - loss: 3.0140 - val_accuracy: 0.6103 - val_loss: 3.0118\n",
            "Epoch 9/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6050 - loss: 3.0098 - val_accuracy: 0.6117 - val_loss: 3.0085\n",
            "Epoch 10/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6084 - loss: 3.0032 - val_accuracy: 0.6171 - val_loss: 3.0016\n",
            "Epoch 11/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6106 - loss: 3.0023 - val_accuracy: 0.6145 - val_loss: 3.0006\n",
            "Epoch 12/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6156 - loss: 2.9958 - val_accuracy: 0.6163 - val_loss: 2.9984\n",
            "Epoch 13/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6213 - loss: 2.9947 - val_accuracy: 0.6208 - val_loss: 2.9934\n",
            "Epoch 14/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6225 - loss: 2.9904 - val_accuracy: 0.6293 - val_loss: 2.9894\n",
            "Epoch 15/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6265 - loss: 2.9874 - val_accuracy: 0.6291 - val_loss: 2.9881\n",
            "Epoch 16/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6272 - loss: 2.9866 - val_accuracy: 0.6316 - val_loss: 2.9880\n",
            "Epoch 17/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6298 - loss: 2.9816 - val_accuracy: 0.6315 - val_loss: 2.9853\n",
            "Epoch 18/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6287 - loss: 2.9836 - val_accuracy: 0.6266 - val_loss: 2.9852\n",
            "Epoch 19/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6302 - loss: 2.9808 - val_accuracy: 0.6284 - val_loss: 2.9832\n",
            "Epoch 20/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6299 - loss: 2.9805 - val_accuracy: 0.6349 - val_loss: 2.9808\n",
            "Epoch 21/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6339 - loss: 2.9755 - val_accuracy: 0.6325 - val_loss: 2.9790\n",
            "Epoch 22/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.6331 - loss: 2.9749 - val_accuracy: 0.6387 - val_loss: 2.9777\n",
            "Epoch 23/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6328 - loss: 2.9775 - val_accuracy: 0.6342 - val_loss: 2.9764\n",
            "Epoch 24/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6360 - loss: 2.9724 - val_accuracy: 0.6399 - val_loss: 2.9776\n",
            "Epoch 25/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6363 - loss: 2.9743 - val_accuracy: 0.6344 - val_loss: 2.9792\n",
            "Epoch 26/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6345 - loss: 2.9747 - val_accuracy: 0.6364 - val_loss: 2.9766\n",
            "Epoch 27/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6359 - loss: 2.9729 - val_accuracy: 0.6331 - val_loss: 2.9782\n",
            "Epoch 28/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6369 - loss: 2.9742 - val_accuracy: 0.6418 - val_loss: 2.9759\n",
            "Epoch 29/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6381 - loss: 2.9710 - val_accuracy: 0.6365 - val_loss: 2.9753\n",
            "Epoch 30/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6373 - loss: 2.9703 - val_accuracy: 0.6340 - val_loss: 2.9750\n",
            "Student val ACC: 0.6418383717536926\n",
            "\n",
            "Training student with layers: [64] activation: relu\n",
            "Epoch 1/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.1218 - loss: 18.1164 - val_accuracy: 0.2794 - val_loss: 3.5941\n",
            "Epoch 2/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.3155 - loss: 3.4278 - val_accuracy: 0.4027 - val_loss: 3.2557\n",
            "Epoch 3/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.4163 - loss: 3.2308 - val_accuracy: 0.4487 - val_loss: 3.1791\n",
            "Epoch 4/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.4598 - loss: 3.1646 - val_accuracy: 0.5029 - val_loss: 3.1313\n",
            "Epoch 5/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5110 - loss: 3.1188 - val_accuracy: 0.5216 - val_loss: 3.1032\n",
            "Epoch 6/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5285 - loss: 3.0947 - val_accuracy: 0.5421 - val_loss: 3.0826\n",
            "Epoch 7/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5473 - loss: 3.0772 - val_accuracy: 0.5622 - val_loss: 3.0626\n",
            "Epoch 8/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5655 - loss: 3.0555 - val_accuracy: 0.5740 - val_loss: 3.0467\n",
            "Epoch 9/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5801 - loss: 3.0423 - val_accuracy: 0.5912 - val_loss: 3.0305\n",
            "Epoch 10/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5901 - loss: 3.0259 - val_accuracy: 0.5983 - val_loss: 3.0201\n",
            "Epoch 11/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5989 - loss: 3.0171 - val_accuracy: 0.6099 - val_loss: 3.0131\n",
            "Epoch 12/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.6063 - loss: 3.0090 - val_accuracy: 0.6141 - val_loss: 3.0082\n",
            "Epoch 13/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6118 - loss: 3.0031 - val_accuracy: 0.6142 - val_loss: 3.0043\n",
            "Epoch 14/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6157 - loss: 3.0010 - val_accuracy: 0.6200 - val_loss: 2.9992\n",
            "Epoch 15/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6196 - loss: 2.9993 - val_accuracy: 0.6225 - val_loss: 2.9972\n",
            "Epoch 16/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.6191 - loss: 2.9934 - val_accuracy: 0.6229 - val_loss: 2.9931\n",
            "Epoch 17/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6247 - loss: 2.9878 - val_accuracy: 0.6318 - val_loss: 2.9836\n",
            "Epoch 18/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6308 - loss: 2.9801 - val_accuracy: 0.6417 - val_loss: 2.9711\n",
            "Epoch 19/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6418 - loss: 2.9685 - val_accuracy: 0.6513 - val_loss: 2.9637\n",
            "Epoch 20/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6484 - loss: 2.9598 - val_accuracy: 0.6542 - val_loss: 2.9578\n",
            "Epoch 21/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6557 - loss: 2.9549 - val_accuracy: 0.6662 - val_loss: 2.9490\n",
            "Epoch 22/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6642 - loss: 2.9487 - val_accuracy: 0.6687 - val_loss: 2.9455\n",
            "Epoch 23/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6693 - loss: 2.9416 - val_accuracy: 0.6723 - val_loss: 2.9372\n",
            "Epoch 24/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6743 - loss: 2.9342 - val_accuracy: 0.6820 - val_loss: 2.9325\n",
            "Epoch 25/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6780 - loss: 2.9325 - val_accuracy: 0.6843 - val_loss: 2.9323\n",
            "Epoch 26/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6800 - loss: 2.9286 - val_accuracy: 0.6804 - val_loss: 2.9326\n",
            "Epoch 27/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6807 - loss: 2.9310 - val_accuracy: 0.6886 - val_loss: 2.9263\n",
            "Epoch 28/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6859 - loss: 2.9247 - val_accuracy: 0.6923 - val_loss: 2.9272\n",
            "Epoch 29/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.6868 - loss: 2.9231 - val_accuracy: 0.6886 - val_loss: 2.9237\n",
            "Epoch 30/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6881 - loss: 2.9228 - val_accuracy: 0.6857 - val_loss: 2.9230\n",
            "Student val ACC: 0.6922927498817444\n",
            "\n",
            "Training student with layers: [64, 32] activation: tanh\n",
            "Epoch 1/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.2492 - loss: 3.4761 - val_accuracy: 0.4974 - val_loss: 3.1307\n",
            "Epoch 2/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.5228 - loss: 3.1063 - val_accuracy: 0.5637 - val_loss: 3.0583\n",
            "Epoch 3/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5680 - loss: 3.0483 - val_accuracy: 0.5834 - val_loss: 3.0313\n",
            "Epoch 4/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5876 - loss: 3.0242 - val_accuracy: 0.6042 - val_loss: 3.0136\n",
            "Epoch 5/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6008 - loss: 3.0103 - val_accuracy: 0.6103 - val_loss: 3.0014\n",
            "Epoch 6/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6101 - loss: 2.9962 - val_accuracy: 0.6210 - val_loss: 2.9918\n",
            "Epoch 7/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6155 - loss: 2.9898 - val_accuracy: 0.6194 - val_loss: 2.9877\n",
            "Epoch 8/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6230 - loss: 2.9805 - val_accuracy: 0.6337 - val_loss: 2.9765\n",
            "Epoch 9/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6276 - loss: 2.9766 - val_accuracy: 0.6345 - val_loss: 2.9761\n",
            "Epoch 10/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.6295 - loss: 2.9750 - val_accuracy: 0.6382 - val_loss: 2.9698\n",
            "Epoch 11/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6345 - loss: 2.9678 - val_accuracy: 0.6383 - val_loss: 2.9679\n",
            "Epoch 12/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6380 - loss: 2.9644 - val_accuracy: 0.6455 - val_loss: 2.9658\n",
            "Epoch 13/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6384 - loss: 2.9621 - val_accuracy: 0.6421 - val_loss: 2.9644\n",
            "Epoch 14/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6433 - loss: 2.9577 - val_accuracy: 0.6438 - val_loss: 2.9606\n",
            "Epoch 15/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6472 - loss: 2.9552 - val_accuracy: 0.6493 - val_loss: 2.9589\n",
            "Epoch 16/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6494 - loss: 2.9529 - val_accuracy: 0.6529 - val_loss: 2.9543\n",
            "Epoch 17/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6501 - loss: 2.9489 - val_accuracy: 0.6562 - val_loss: 2.9505\n",
            "Epoch 18/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6511 - loss: 2.9503 - val_accuracy: 0.6510 - val_loss: 2.9544\n",
            "Epoch 19/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6554 - loss: 2.9459 - val_accuracy: 0.6552 - val_loss: 2.9472\n",
            "Epoch 20/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6567 - loss: 2.9447 - val_accuracy: 0.6556 - val_loss: 2.9497\n",
            "Epoch 21/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6577 - loss: 2.9438 - val_accuracy: 0.6592 - val_loss: 2.9466\n",
            "Epoch 22/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6601 - loss: 2.9432 - val_accuracy: 0.6573 - val_loss: 2.9453\n",
            "Epoch 23/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6589 - loss: 2.9423 - val_accuracy: 0.6611 - val_loss: 2.9424\n",
            "Epoch 24/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6629 - loss: 2.9391 - val_accuracy: 0.6595 - val_loss: 2.9435\n",
            "Epoch 25/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6616 - loss: 2.9396 - val_accuracy: 0.6641 - val_loss: 2.9413\n",
            "Epoch 26/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6633 - loss: 2.9366 - val_accuracy: 0.6644 - val_loss: 2.9385\n",
            "Epoch 27/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6636 - loss: 2.9348 - val_accuracy: 0.6645 - val_loss: 2.9399\n",
            "Epoch 28/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6650 - loss: 2.9377 - val_accuracy: 0.6693 - val_loss: 2.9384\n",
            "Epoch 29/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6638 - loss: 2.9346 - val_accuracy: 0.6701 - val_loss: 2.9361\n",
            "Epoch 30/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6653 - loss: 2.9328 - val_accuracy: 0.6717 - val_loss: 2.9341\n",
            "Student val ACC: 0.671674370765686\n",
            "\n",
            "Training student with layers: [64, 32] activation: relu\n",
            "Epoch 1/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.1143 - loss: 6.6020 - val_accuracy: 0.3329 - val_loss: 3.2901\n",
            "Epoch 2/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.3758 - loss: 3.2459 - val_accuracy: 0.4922 - val_loss: 3.1333\n",
            "Epoch 3/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5055 - loss: 3.1209 - val_accuracy: 0.5355 - val_loss: 3.0973\n",
            "Epoch 4/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.5377 - loss: 3.0909 - val_accuracy: 0.5575 - val_loss: 3.0777\n",
            "Epoch 5/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.5535 - loss: 3.0745 - val_accuracy: 0.5666 - val_loss: 3.0634\n",
            "Epoch 6/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5647 - loss: 3.0593 - val_accuracy: 0.5671 - val_loss: 3.0518\n",
            "Epoch 7/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5803 - loss: 3.0460 - val_accuracy: 0.5974 - val_loss: 3.0344\n",
            "Epoch 8/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.5996 - loss: 3.0303 - val_accuracy: 0.6145 - val_loss: 3.0195\n",
            "Epoch 9/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6138 - loss: 3.0152 - val_accuracy: 0.6251 - val_loss: 3.0022\n",
            "Epoch 10/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6290 - loss: 2.9956 - val_accuracy: 0.6377 - val_loss: 2.9906\n",
            "Epoch 11/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6391 - loss: 2.9840 - val_accuracy: 0.6466 - val_loss: 2.9783\n",
            "Epoch 12/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6465 - loss: 2.9723 - val_accuracy: 0.6529 - val_loss: 2.9695\n",
            "Epoch 13/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.6522 - loss: 2.9647 - val_accuracy: 0.6624 - val_loss: 2.9607\n",
            "Epoch 14/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6605 - loss: 2.9549 - val_accuracy: 0.6666 - val_loss: 2.9508\n",
            "Epoch 15/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6661 - loss: 2.9448 - val_accuracy: 0.6698 - val_loss: 2.9417\n",
            "Epoch 16/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.6734 - loss: 2.9361 - val_accuracy: 0.6834 - val_loss: 2.9338\n",
            "Epoch 17/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.6775 - loss: 2.9320 - val_accuracy: 0.6877 - val_loss: 2.9297\n",
            "Epoch 18/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6850 - loss: 2.9251 - val_accuracy: 0.6908 - val_loss: 2.9218\n",
            "Epoch 19/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.6927 - loss: 2.9204 - val_accuracy: 0.7011 - val_loss: 2.9173\n",
            "Epoch 20/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6958 - loss: 2.9164 - val_accuracy: 0.7059 - val_loss: 2.9145\n",
            "Epoch 21/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6984 - loss: 2.9109 - val_accuracy: 0.7005 - val_loss: 2.9112\n",
            "Epoch 22/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7014 - loss: 2.9079 - val_accuracy: 0.7091 - val_loss: 2.9078\n",
            "Epoch 23/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7031 - loss: 2.9060 - val_accuracy: 0.7035 - val_loss: 2.9063\n",
            "Epoch 24/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.7075 - loss: 2.9013 - val_accuracy: 0.7088 - val_loss: 2.9025\n",
            "Epoch 25/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7094 - loss: 2.8978 - val_accuracy: 0.7159 - val_loss: 2.8974\n",
            "Epoch 26/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7123 - loss: 2.8968 - val_accuracy: 0.7219 - val_loss: 2.8947\n",
            "Epoch 27/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7167 - loss: 2.8919 - val_accuracy: 0.7185 - val_loss: 2.8934\n",
            "Epoch 28/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7184 - loss: 2.8903 - val_accuracy: 0.7214 - val_loss: 2.8895\n",
            "Epoch 29/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.7191 - loss: 2.8859 - val_accuracy: 0.7236 - val_loss: 2.8860\n",
            "Epoch 30/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7206 - loss: 2.8848 - val_accuracy: 0.7254 - val_loss: 2.8825\n",
            "Student val ACC: 0.7254438996315002\n",
            "\n",
            "Training student with layers: [128, 64, 32] activation: tanh\n",
            "Epoch 1/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.3659 - loss: 3.3441 - val_accuracy: 0.6231 - val_loss: 3.0068\n",
            "Epoch 2/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6292 - loss: 2.9873 - val_accuracy: 0.6572 - val_loss: 2.9493\n",
            "Epoch 3/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6645 - loss: 2.9369 - val_accuracy: 0.6828 - val_loss: 2.9212\n",
            "Epoch 4/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6834 - loss: 2.9171 - val_accuracy: 0.6978 - val_loss: 2.9080\n",
            "Epoch 5/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6971 - loss: 2.9025 - val_accuracy: 0.7083 - val_loss: 2.8971\n",
            "Epoch 6/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7075 - loss: 2.8938 - val_accuracy: 0.7177 - val_loss: 2.8875\n",
            "Epoch 7/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7114 - loss: 2.8884 - val_accuracy: 0.7129 - val_loss: 2.8881\n",
            "Epoch 8/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7136 - loss: 2.8841 - val_accuracy: 0.7251 - val_loss: 2.8811\n",
            "Epoch 9/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7188 - loss: 2.8797 - val_accuracy: 0.7227 - val_loss: 2.8788\n",
            "Epoch 10/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7259 - loss: 2.8727 - val_accuracy: 0.7301 - val_loss: 2.8735\n",
            "Epoch 11/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.7277 - loss: 2.8714 - val_accuracy: 0.7351 - val_loss: 2.8709\n",
            "Epoch 12/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.7316 - loss: 2.8663 - val_accuracy: 0.7340 - val_loss: 2.8682\n",
            "Epoch 13/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7326 - loss: 2.8655 - val_accuracy: 0.7403 - val_loss: 2.8676\n",
            "Epoch 14/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7332 - loss: 2.8643 - val_accuracy: 0.7329 - val_loss: 2.8669\n",
            "Epoch 15/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.7353 - loss: 2.8628 - val_accuracy: 0.7423 - val_loss: 2.8646\n",
            "Epoch 16/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7378 - loss: 2.8611 - val_accuracy: 0.7429 - val_loss: 2.8649\n",
            "Epoch 17/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7388 - loss: 2.8578 - val_accuracy: 0.7442 - val_loss: 2.8598\n",
            "Epoch 18/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7405 - loss: 2.8594 - val_accuracy: 0.7414 - val_loss: 2.8630\n",
            "Epoch 19/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7413 - loss: 2.8563 - val_accuracy: 0.7460 - val_loss: 2.8584\n",
            "Epoch 20/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7435 - loss: 2.8547 - val_accuracy: 0.7459 - val_loss: 2.8578\n",
            "Epoch 21/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.7443 - loss: 2.8538 - val_accuracy: 0.7437 - val_loss: 2.8586\n",
            "Epoch 22/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7432 - loss: 2.8557 - val_accuracy: 0.7476 - val_loss: 2.8561\n",
            "Epoch 23/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7462 - loss: 2.8518 - val_accuracy: 0.7492 - val_loss: 2.8545\n",
            "Epoch 24/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.7468 - loss: 2.8519 - val_accuracy: 0.7537 - val_loss: 2.8533\n",
            "Epoch 25/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7477 - loss: 2.8511 - val_accuracy: 0.7536 - val_loss: 2.8526\n",
            "Epoch 26/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7480 - loss: 2.8498 - val_accuracy: 0.7509 - val_loss: 2.8526\n",
            "Epoch 27/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7482 - loss: 2.8485 - val_accuracy: 0.7472 - val_loss: 2.8546\n",
            "Epoch 28/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7490 - loss: 2.8480 - val_accuracy: 0.7516 - val_loss: 2.8531\n",
            "Epoch 29/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7493 - loss: 2.8488 - val_accuracy: 0.7513 - val_loss: 2.8508\n",
            "Epoch 30/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.7531 - loss: 2.8452 - val_accuracy: 0.7494 - val_loss: 2.8511\n",
            "Student val ACC: 0.753670871257782\n",
            "\n",
            "Training student with layers: [128, 64, 32] activation: relu\n",
            "Epoch 1/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.1215 - loss: 5.7503 - val_accuracy: 0.3575 - val_loss: 3.2648\n",
            "Epoch 2/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.4135 - loss: 3.2122 - val_accuracy: 0.5273 - val_loss: 3.1009\n",
            "Epoch 3/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5519 - loss: 3.0789 - val_accuracy: 0.5945 - val_loss: 3.0359\n",
            "Epoch 4/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6013 - loss: 3.0297 - val_accuracy: 0.6132 - val_loss: 3.0154\n",
            "Epoch 5/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6201 - loss: 3.0050 - val_accuracy: 0.6406 - val_loss: 2.9927\n",
            "Epoch 6/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.6356 - loss: 2.9895 - val_accuracy: 0.6468 - val_loss: 2.9796\n",
            "Epoch 7/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6489 - loss: 2.9738 - val_accuracy: 0.6669 - val_loss: 2.9647\n",
            "Epoch 8/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.6614 - loss: 2.9604 - val_accuracy: 0.6736 - val_loss: 2.9552\n",
            "Epoch 9/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.6707 - loss: 2.9518 - val_accuracy: 0.6807 - val_loss: 2.9433\n",
            "Epoch 10/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6784 - loss: 2.9404 - val_accuracy: 0.6939 - val_loss: 2.9321\n",
            "Epoch 11/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6860 - loss: 2.9281 - val_accuracy: 0.6961 - val_loss: 2.9229\n",
            "Epoch 12/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.6925 - loss: 2.9195 - val_accuracy: 0.6985 - val_loss: 2.9225\n",
            "Epoch 13/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.6969 - loss: 2.9116 - val_accuracy: 0.7090 - val_loss: 2.9098\n",
            "Epoch 14/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7038 - loss: 2.9069 - val_accuracy: 0.7153 - val_loss: 2.9074\n",
            "Epoch 15/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.7090 - loss: 2.9008 - val_accuracy: 0.7132 - val_loss: 2.8987\n",
            "Epoch 16/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7136 - loss: 2.8945 - val_accuracy: 0.7130 - val_loss: 2.8954\n",
            "Epoch 17/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.7138 - loss: 2.8939 - val_accuracy: 0.7254 - val_loss: 2.8905\n",
            "Epoch 18/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7167 - loss: 2.8890 - val_accuracy: 0.7227 - val_loss: 2.8875\n",
            "Epoch 19/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.7218 - loss: 2.8832 - val_accuracy: 0.7233 - val_loss: 2.8834\n",
            "Epoch 20/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7245 - loss: 2.8798 - val_accuracy: 0.7346 - val_loss: 2.8769\n",
            "Epoch 21/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7297 - loss: 2.8750 - val_accuracy: 0.7394 - val_loss: 2.8704\n",
            "Epoch 22/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7352 - loss: 2.8682 - val_accuracy: 0.7435 - val_loss: 2.8635\n",
            "Epoch 23/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7409 - loss: 2.8612 - val_accuracy: 0.7449 - val_loss: 2.8614\n",
            "Epoch 24/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7445 - loss: 2.8563 - val_accuracy: 0.7493 - val_loss: 2.8559\n",
            "Epoch 25/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.7486 - loss: 2.8515 - val_accuracy: 0.7586 - val_loss: 2.8550\n",
            "Epoch 26/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7518 - loss: 2.8498 - val_accuracy: 0.7643 - val_loss: 2.8504\n",
            "Epoch 27/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7551 - loss: 2.8490 - val_accuracy: 0.7664 - val_loss: 2.8485\n",
            "Epoch 28/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.7597 - loss: 2.8428 - val_accuracy: 0.7673 - val_loss: 2.8463\n",
            "Epoch 29/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7622 - loss: 2.8438 - val_accuracy: 0.7710 - val_loss: 2.8440\n",
            "Epoch 30/30\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.7643 - loss: 2.8417 - val_accuracy: 0.7688 - val_loss: 2.8416\n",
            "Student val ACC: 0.7709660530090332\n",
            "\n",
            "Best student val ACC: 0.7709660530090332 Config: ([128, 64, 32], 'relu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Finetune baseline MLP\n",
        "baseline_configs = [\n",
        "    (64, 32),\n",
        "    (128, 64),\n",
        "    (256, 128, 64)\n",
        "]\n",
        "activations_baseline = [\"tanh\", \"relu\"]\n",
        "\n",
        "best_val_acc_baseline = 0\n",
        "best_baseline = None\n",
        "best_baseline_cfg = None\n",
        "\n",
        "for cfg in baseline_configs:\n",
        "    for act in activations_baseline:\n",
        "        print(\"\\nTraining baseline with layers:\", cfg, \"activation:\", act)\n",
        "        baseline = MLPClassifier(\n",
        "            hidden_layer_sizes=cfg,\n",
        "            activation=act,\n",
        "            max_iter=30,\n",
        "            random_state=42,\n",
        "            early_stopping = True,\n",
        "            verbose = True\n",
        "        )\n",
        "        baseline.fit(X_tr, y_tr)\n",
        "        pred = baseline.predict(X_va)\n",
        "        val_acc = accuracy_score(y_va, pred)\n",
        "        print(\"Baseline val ACC:\", val_acc)\n",
        "\n",
        "        if val_acc > best_val_acc_baseline:\n",
        "            best_val_acc_baseline = val_acc\n",
        "            best_baseline = baseline\n",
        "            best_baseline_cfg = (cfg, act)\n",
        "\n",
        "print(\"\\nBest baseline val ACC:\", best_val_acc_baseline, \"Config:\", best_baseline_cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9af8ShzhrRh",
        "outputId": "fc7316e4-c0e5-4577-c74c-c7ccc51b90d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training baseline with layers: (64, 32) activation: tanh\n",
            "Iteration 1, loss = 2.17087872\n",
            "Validation score: 0.553600\n",
            "Iteration 2, loss = 1.61118615\n",
            "Validation score: 0.578400\n",
            "Iteration 3, loss = 1.51412658\n",
            "Validation score: 0.603700\n",
            "Iteration 4, loss = 1.45650640\n",
            "Validation score: 0.610250\n",
            "Iteration 5, loss = 1.42074393\n",
            "Validation score: 0.614850\n",
            "Iteration 6, loss = 1.39438086\n",
            "Validation score: 0.625900\n",
            "Iteration 7, loss = 1.37441565\n",
            "Validation score: 0.628850\n",
            "Iteration 8, loss = 1.35326158\n",
            "Validation score: 0.631000\n",
            "Iteration 9, loss = 1.34425216\n",
            "Validation score: 0.633150\n",
            "Iteration 10, loss = 1.33041534\n",
            "Validation score: 0.638150\n",
            "Iteration 11, loss = 1.31325060\n",
            "Validation score: 0.644200\n",
            "Iteration 12, loss = 1.31013708\n",
            "Validation score: 0.654050\n",
            "Iteration 13, loss = 1.29824610\n",
            "Validation score: 0.648400\n",
            "Iteration 14, loss = 1.29367896\n",
            "Validation score: 0.645700\n",
            "Iteration 15, loss = 1.28189944\n",
            "Validation score: 0.643650\n",
            "Iteration 16, loss = 1.27963620\n",
            "Validation score: 0.647100\n",
            "Iteration 17, loss = 1.28177198\n",
            "Validation score: 0.659450\n",
            "Iteration 18, loss = 1.26823724\n",
            "Validation score: 0.657100\n",
            "Iteration 19, loss = 1.26806481\n",
            "Validation score: 0.647550\n",
            "Iteration 20, loss = 1.26636693\n",
            "Validation score: 0.649450\n",
            "Iteration 21, loss = 1.25378760\n",
            "Validation score: 0.655500\n",
            "Iteration 22, loss = 1.25556381\n",
            "Validation score: 0.662250\n",
            "Iteration 23, loss = 1.25431126\n",
            "Validation score: 0.656250\n",
            "Iteration 24, loss = 1.24771349\n",
            "Validation score: 0.657850\n",
            "Iteration 25, loss = 1.24619090\n",
            "Validation score: 0.663100\n",
            "Iteration 26, loss = 1.23906131\n",
            "Validation score: 0.663600\n",
            "Iteration 27, loss = 1.23689820\n",
            "Validation score: 0.653250\n",
            "Iteration 28, loss = 1.23769325\n",
            "Validation score: 0.663650\n",
            "Iteration 29, loss = 1.23653049\n",
            "Validation score: 0.667400\n",
            "Iteration 30, loss = 1.23330793\n",
            "Validation score: 0.656800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline val ACC: 0.6657961124227821\n",
            "\n",
            "Training baseline with layers: (64, 32) activation: relu\n",
            "Iteration 1, loss = 2.68398143\n",
            "Validation score: 0.542050\n",
            "Iteration 2, loss = 1.54712364\n",
            "Validation score: 0.601650\n",
            "Iteration 3, loss = 1.39205333\n",
            "Validation score: 0.632650\n",
            "Iteration 4, loss = 1.30088841\n",
            "Validation score: 0.649800\n",
            "Iteration 5, loss = 1.24836077\n",
            "Validation score: 0.650950\n",
            "Iteration 6, loss = 1.20948451\n",
            "Validation score: 0.661050\n",
            "Iteration 7, loss = 1.17059196\n",
            "Validation score: 0.677850\n",
            "Iteration 8, loss = 1.13981065\n",
            "Validation score: 0.683650\n",
            "Iteration 9, loss = 1.11892492\n",
            "Validation score: 0.688900\n",
            "Iteration 10, loss = 1.09700543\n",
            "Validation score: 0.695550\n",
            "Iteration 11, loss = 1.08351749\n",
            "Validation score: 0.700850\n",
            "Iteration 12, loss = 1.06794333\n",
            "Validation score: 0.695600\n",
            "Iteration 13, loss = 1.05484462\n",
            "Validation score: 0.699850\n",
            "Iteration 14, loss = 1.04666295\n",
            "Validation score: 0.708350\n",
            "Iteration 15, loss = 1.03803979\n",
            "Validation score: 0.708300\n",
            "Iteration 16, loss = 1.02808904\n",
            "Validation score: 0.711600\n",
            "Iteration 17, loss = 1.02086099\n",
            "Validation score: 0.714700\n",
            "Iteration 18, loss = 1.01303909\n",
            "Validation score: 0.713800\n",
            "Iteration 19, loss = 1.00681853\n",
            "Validation score: 0.716650\n",
            "Iteration 20, loss = 1.00139684\n",
            "Validation score: 0.715050\n",
            "Iteration 21, loss = 0.99582671\n",
            "Validation score: 0.721100\n",
            "Iteration 22, loss = 0.99102983\n",
            "Validation score: 0.723300\n",
            "Iteration 23, loss = 0.98916016\n",
            "Validation score: 0.722850\n",
            "Iteration 24, loss = 0.98417456\n",
            "Validation score: 0.724600\n",
            "Iteration 25, loss = 0.98032415\n",
            "Validation score: 0.723450\n",
            "Iteration 26, loss = 0.97656990\n",
            "Validation score: 0.722200\n",
            "Iteration 27, loss = 0.97412478\n",
            "Validation score: 0.723050\n",
            "Iteration 28, loss = 0.97149826\n",
            "Validation score: 0.732150\n",
            "Iteration 29, loss = 0.96988407\n",
            "Validation score: 0.718750\n",
            "Iteration 30, loss = 0.96640627\n",
            "Validation score: 0.725650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline val ACC: 0.7253307028041011\n",
            "\n",
            "Training baseline with layers: (128, 64) activation: tanh\n",
            "Iteration 1, loss = 1.78665419\n",
            "Validation score: 0.603900\n",
            "Iteration 2, loss = 1.40063561\n",
            "Validation score: 0.631400\n",
            "Iteration 3, loss = 1.32438565\n",
            "Validation score: 0.643050\n",
            "Iteration 4, loss = 1.28337698\n",
            "Validation score: 0.650800\n",
            "Iteration 5, loss = 1.25667911\n",
            "Validation score: 0.654500\n",
            "Iteration 6, loss = 1.22953114\n",
            "Validation score: 0.657750\n",
            "Iteration 7, loss = 1.21370072\n",
            "Validation score: 0.663250\n",
            "Iteration 8, loss = 1.20164697\n",
            "Validation score: 0.667350\n",
            "Iteration 9, loss = 1.18178704\n",
            "Validation score: 0.669200\n",
            "Iteration 10, loss = 1.17729666\n",
            "Validation score: 0.668800\n",
            "Iteration 11, loss = 1.17238577\n",
            "Validation score: 0.679300\n",
            "Iteration 12, loss = 1.15721857\n",
            "Validation score: 0.688250\n",
            "Iteration 13, loss = 1.14824891\n",
            "Validation score: 0.681550\n",
            "Iteration 14, loss = 1.14724765\n",
            "Validation score: 0.681650\n",
            "Iteration 15, loss = 1.14587670\n",
            "Validation score: 0.677300\n",
            "Iteration 16, loss = 1.14017883\n",
            "Validation score: 0.682550\n",
            "Iteration 17, loss = 1.13696683\n",
            "Validation score: 0.680400\n",
            "Iteration 18, loss = 1.13469791\n",
            "Validation score: 0.684450\n",
            "Iteration 19, loss = 1.14033950\n",
            "Validation score: 0.690500\n",
            "Iteration 20, loss = 1.14038468\n",
            "Validation score: 0.690500\n",
            "Iteration 21, loss = 1.13689254\n",
            "Validation score: 0.683150\n",
            "Iteration 22, loss = 1.12882162\n",
            "Validation score: 0.689450\n",
            "Iteration 23, loss = 1.12427900\n",
            "Validation score: 0.685850\n",
            "Iteration 24, loss = 1.12315441\n",
            "Validation score: 0.678450\n",
            "Iteration 25, loss = 1.11296311\n",
            "Validation score: 0.690050\n",
            "Iteration 26, loss = 1.11403946\n",
            "Validation score: 0.695550\n",
            "Iteration 27, loss = 1.11669425\n",
            "Validation score: 0.686100\n",
            "Iteration 28, loss = 1.11484182\n",
            "Validation score: 0.688500\n",
            "Iteration 29, loss = 1.10550542\n",
            "Validation score: 0.694650\n",
            "Iteration 30, loss = 1.10471653\n",
            "Validation score: 0.688850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline val ACC: 0.6915731427277726\n",
            "\n",
            "Training baseline with layers: (128, 64) activation: relu\n",
            "Iteration 1, loss = 2.06409938\n",
            "Validation score: 0.633000\n",
            "Iteration 2, loss = 1.26837826\n",
            "Validation score: 0.669050\n",
            "Iteration 3, loss = 1.16872843\n",
            "Validation score: 0.676500\n",
            "Iteration 4, loss = 1.11138396\n",
            "Validation score: 0.692200\n",
            "Iteration 5, loss = 1.06630247\n",
            "Validation score: 0.703050\n",
            "Iteration 6, loss = 1.03743659\n",
            "Validation score: 0.698850\n",
            "Iteration 7, loss = 1.01248348\n",
            "Validation score: 0.721900\n",
            "Iteration 8, loss = 0.98943968\n",
            "Validation score: 0.712750\n",
            "Iteration 9, loss = 0.97362714\n",
            "Validation score: 0.725300\n",
            "Iteration 10, loss = 0.96335984\n",
            "Validation score: 0.731250\n",
            "Iteration 11, loss = 0.94495634\n",
            "Validation score: 0.721000\n",
            "Iteration 12, loss = 0.93732030\n",
            "Validation score: 0.733800\n",
            "Iteration 13, loss = 0.92699007\n",
            "Validation score: 0.738950\n",
            "Iteration 14, loss = 0.91764378\n",
            "Validation score: 0.743500\n",
            "Iteration 15, loss = 0.90841855\n",
            "Validation score: 0.742650\n",
            "Iteration 16, loss = 0.90191450\n",
            "Validation score: 0.745450\n",
            "Iteration 17, loss = 0.89491486\n",
            "Validation score: 0.739800\n",
            "Iteration 18, loss = 0.89043195\n",
            "Validation score: 0.745850\n",
            "Iteration 19, loss = 0.88525639\n",
            "Validation score: 0.749500\n",
            "Iteration 20, loss = 0.88380191\n",
            "Validation score: 0.740400\n",
            "Iteration 21, loss = 0.87633707\n",
            "Validation score: 0.741800\n",
            "Iteration 22, loss = 0.87468097\n",
            "Validation score: 0.751100\n",
            "Iteration 23, loss = 0.86754387\n",
            "Validation score: 0.740750\n",
            "Iteration 24, loss = 0.86542207\n",
            "Validation score: 0.744850\n",
            "Iteration 25, loss = 0.86003415\n",
            "Validation score: 0.752100\n",
            "Iteration 26, loss = 0.85505987\n",
            "Validation score: 0.744800\n",
            "Iteration 27, loss = 0.85478065\n",
            "Validation score: 0.749700\n",
            "Iteration 28, loss = 0.85248038\n",
            "Validation score: 0.746100\n",
            "Iteration 29, loss = 0.84697094\n",
            "Validation score: 0.755000\n",
            "Iteration 30, loss = 0.84641688\n",
            "Validation score: 0.746950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline val ACC: 0.7531372295352372\n",
            "\n",
            "Training baseline with layers: (256, 128, 64) activation: tanh\n",
            "Iteration 1, loss = 1.61158930\n",
            "Validation score: 0.613100\n",
            "Iteration 2, loss = 1.31614159\n",
            "Validation score: 0.644150\n",
            "Iteration 3, loss = 1.25508029\n",
            "Validation score: 0.662600\n",
            "Iteration 4, loss = 1.22645305\n",
            "Validation score: 0.673300\n",
            "Iteration 5, loss = 1.20397580\n",
            "Validation score: 0.672000\n",
            "Iteration 6, loss = 1.16112086\n",
            "Validation score: 0.672100\n",
            "Iteration 7, loss = 1.15611683\n",
            "Validation score: 0.677850\n",
            "Iteration 8, loss = 1.13686688\n",
            "Validation score: 0.681050\n",
            "Iteration 9, loss = 1.12817175\n",
            "Validation score: 0.679450\n",
            "Iteration 10, loss = 1.12528787\n",
            "Validation score: 0.676800\n",
            "Iteration 11, loss = 1.11589856\n",
            "Validation score: 0.678250\n",
            "Iteration 12, loss = 1.11863576\n",
            "Validation score: 0.678300\n",
            "Iteration 13, loss = 1.10265782\n",
            "Validation score: 0.682850\n",
            "Iteration 14, loss = 1.09646532\n",
            "Validation score: 0.682600\n",
            "Iteration 15, loss = 1.09234324\n",
            "Validation score: 0.678750\n",
            "Iteration 16, loss = 1.09039627\n",
            "Validation score: 0.680800\n",
            "Iteration 17, loss = 1.08745817\n",
            "Validation score: 0.690900\n",
            "Iteration 18, loss = 1.08048741\n",
            "Validation score: 0.699050\n",
            "Iteration 19, loss = 1.07839885\n",
            "Validation score: 0.695550\n",
            "Iteration 20, loss = 1.07726720\n",
            "Validation score: 0.686700\n",
            "Iteration 21, loss = 1.07337982\n",
            "Validation score: 0.685000\n",
            "Iteration 22, loss = 1.06343747\n",
            "Validation score: 0.697800\n",
            "Iteration 23, loss = 1.07037850\n",
            "Validation score: 0.692100\n",
            "Iteration 24, loss = 1.06716644\n",
            "Validation score: 0.699600\n",
            "Iteration 25, loss = 1.06063613\n",
            "Validation score: 0.696800\n",
            "Iteration 26, loss = 1.05820163\n",
            "Validation score: 0.690300\n",
            "Iteration 27, loss = 1.06975219\n",
            "Validation score: 0.693750\n",
            "Iteration 28, loss = 1.06193013\n",
            "Validation score: 0.694600\n",
            "Iteration 29, loss = 1.05991686\n",
            "Validation score: 0.696500\n",
            "Iteration 30, loss = 1.05392432\n",
            "Validation score: 0.703650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline val ACC: 0.7050195672563796\n",
            "\n",
            "Training baseline with layers: (256, 128, 64) activation: relu\n",
            "Iteration 1, loss = 1.80567255\n",
            "Validation score: 0.644700\n",
            "Iteration 2, loss = 1.19027189\n",
            "Validation score: 0.680550\n",
            "Iteration 3, loss = 1.08375042\n",
            "Validation score: 0.704200\n",
            "Iteration 4, loss = 1.01983941\n",
            "Validation score: 0.710400\n",
            "Iteration 5, loss = 0.98385671\n",
            "Validation score: 0.724750\n",
            "Iteration 6, loss = 0.95174610\n",
            "Validation score: 0.725100\n",
            "Iteration 7, loss = 0.92625217\n",
            "Validation score: 0.739350\n",
            "Iteration 8, loss = 0.90597851\n",
            "Validation score: 0.738100\n",
            "Iteration 9, loss = 0.89175025\n",
            "Validation score: 0.746200\n",
            "Iteration 10, loss = 0.87444646\n",
            "Validation score: 0.745200\n",
            "Iteration 11, loss = 0.86432308\n",
            "Validation score: 0.746550\n",
            "Iteration 12, loss = 0.85465026\n",
            "Validation score: 0.752000\n",
            "Iteration 13, loss = 0.84428132\n",
            "Validation score: 0.747850\n",
            "Iteration 14, loss = 0.83205903\n",
            "Validation score: 0.758400\n",
            "Iteration 15, loss = 0.82696149\n",
            "Validation score: 0.744350\n",
            "Iteration 16, loss = 0.81829562\n",
            "Validation score: 0.752200\n",
            "Iteration 17, loss = 0.81002163\n",
            "Validation score: 0.765250\n",
            "Iteration 18, loss = 0.80532659\n",
            "Validation score: 0.758600\n",
            "Iteration 19, loss = 0.80016798\n",
            "Validation score: 0.759700\n",
            "Iteration 20, loss = 0.79210876\n",
            "Validation score: 0.762350\n",
            "Iteration 21, loss = 0.78998330\n",
            "Validation score: 0.760450\n",
            "Iteration 22, loss = 0.78230775\n",
            "Validation score: 0.760800\n",
            "Iteration 23, loss = 0.78143132\n",
            "Validation score: 0.764350\n",
            "Iteration 24, loss = 0.77137207\n",
            "Validation score: 0.764600\n",
            "Iteration 25, loss = 0.77203899\n",
            "Validation score: 0.771400\n",
            "Iteration 26, loss = 0.76334582\n",
            "Validation score: 0.767550\n",
            "Iteration 27, loss = 0.76033663\n",
            "Validation score: 0.768500\n",
            "Iteration 28, loss = 0.75413123\n",
            "Validation score: 0.771100\n",
            "Iteration 29, loss = 0.75380923\n",
            "Validation score: 0.768900\n",
            "Iteration 30, loss = 0.74772050\n",
            "Validation score: 0.768100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline val ACC: 0.7722274329700184\n",
            "\n",
            "Best baseline val ACC: 0.7722274329700184 Config: ((256, 128, 64), 'relu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export best baseline MLP weight to .h\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "HEADER_FILE = \"mlp_baseline_weights.h\"\n",
        "PREFIX = \"MLP_BASELINE\"\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "x_max = np.percentile(np.abs(X_scaled), 99.9)\n",
        "INPUT_SCALE = 127.0 / x_max\n",
        "\n",
        "mlp_best = best_baseline\n",
        "\n",
        "coefs = mlp_best.coefs_            # list of (in_dim, out_dim)\n",
        "intercepts = mlp_best.intercepts_  # list of (out_dim,)\n",
        "\n",
        "with open(HEADER_FILE, \"w\") as f:\n",
        "    f.write(\"#pragma once\\n\\n\")\n",
        "    f.write(\"#include <stdint.h>\\n\\n\")\n",
        "\n",
        "    f.write(\"// ===== Quantization config =====\\n\")\n",
        "    f.write(f\"#define {PREFIX}_INPUT_SCALE {INPUT_SCALE:.8f}f\\n\\n\")\n",
        "\n",
        "    prev_scale = INPUT_SCALE\n",
        "\n",
        "    for layer_idx, (W, b) in enumerate(zip(coefs, intercepts)):\n",
        "        in_dim, out_dim = W.shape\n",
        "\n",
        "        # transpose cho C: (out_dim, in_dim)\n",
        "        W = W.T\n",
        "\n",
        "        # ---- Quantize weights sang int8 ----\n",
        "        W_max = np.max(np.abs(W))\n",
        "        W_scale = 127.0 / W_max if W_max != 0 else 1.0\n",
        "        W_q = np.round(W * W_scale).astype(np.int8)\n",
        "\n",
        "        # ---- Quantize bias sang int32 ----\n",
        "        b_scale = prev_scale * W_scale\n",
        "        b_q = np.round(b * b_scale).astype(np.int32)\n",
        "\n",
        "        # ---- Write metadata ----\n",
        "        f.write(f\"// ===== Layer {layer_idx} =====\\n\")\n",
        "        f.write(f\"#define {PREFIX}_L{layer_idx}_IN  {in_dim}\\n\")\n",
        "        f.write(f\"#define {PREFIX}_L{layer_idx}_OUT {out_dim}\\n\")\n",
        "        f.write(f\"#define {PREFIX}_L{layer_idx}_W_SCALE {W_scale:.8f}f\\n\")\n",
        "        f.write(f\"#define {PREFIX}_L{layer_idx}_B_SCALE {b_scale:.8f}f\\n\\n\")\n",
        "\n",
        "        # ---- Write weights ----\n",
        "        f.write(f\"const int8_t {PREFIX}_W{layer_idx}[{out_dim}][{in_dim}] = {{\\n\")\n",
        "        for row in W_q:\n",
        "            f.write(\"  { \" + \", \".join(map(str, row)) + \" },\\n\")\n",
        "        f.write(\"};\\n\\n\")\n",
        "\n",
        "        # ---- Write biases ----\n",
        "        f.write(f\"const int32_t {PREFIX}_B{layer_idx}[{out_dim}] = {{\\n\")\n",
        "        f.write(\"  \" + \", \".join(map(str, b_q)) + \"\\n\")\n",
        "        f.write(\"};\\n\\n\")\n",
        "\n",
        "        prev_scale = b_scale\n",
        "\n",
        "print(f\"Exported quantized baseline MLP weights to {HEADER_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQdwu4dK47u3",
        "outputId": "b8086b00-d8c0-4b66-d5ef-a7987ef5d4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported quantized baseline MLP weights to mlp_baseline_weights.h\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2633043613.py:40: RuntimeWarning: invalid value encountered in cast\n",
            "  b_q = np.round(b * b_scale).astype(np.int32)\n"
          ]
        }
      ]
    }
  ]
}